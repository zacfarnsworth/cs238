{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading Cairo backend into Compose.jl\n",
      "└ @ Compose C:\\Users\\zacfa\\.julia\\packages\\Compose\\BYWXX\\src\\Compose.jl:161\n",
      "┌ Warning: Package Compose does not have Cairo in its dependencies:\n",
      "│ - If you have Compose checked out for development and have\n",
      "│   added Cairo as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with Compose\n",
      "│ Loading Cairo into Compose from project dependency, future warnings for Compose are suppressed.\n",
      "└ @ nothing nothing:837\n"
     ]
    }
   ],
   "source": [
    "# activate project environment\n",
    "# include these lines of code in any future scripts/notebooks\n",
    "#---\n",
    "import Pkg\n",
    "if !haskey(Pkg.installed(), \"AA228FinalProject\")\n",
    "    jenv = joinpath(dirname(@__FILE__()), \"..\") # this assumes the notebook is in the same dir\n",
    "    # as the Project.toml file, which should be in top level dir of the project. \n",
    "    # Change accordingly if this is not the case.\n",
    "    Pkg.activate(jenv)\n",
    "end\n",
    "#---\n",
    "\n",
    "# import necessary packages\n",
    "using AA228FinalProject\n",
    "using POMDPs\n",
    "using POMDPPolicies\n",
    "using BeliefUpdaters\n",
    "using ParticleFilters\n",
    "using POMDPSimulators\n",
    "using Cairo\n",
    "using Gtk\n",
    "using Random\n",
    "using Printf\n",
    "using POMDPModels\n",
    "using POMDPSimulators\n",
    "using QMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = Bumper() # or Bumper() for the bumper version of the environment\n",
    "config = 2 # 1,2, or 3\n",
    "m = RoombaPOMDP(sensor=sensor, mdp=RoombaMDP(config=config));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 2000\n",
    "resampler = BumperResampler(num_particles)\n",
    "\n",
    "spf = SimpleParticleFilter(m, resampler)\n",
    "\n",
    "v_noise_coefficient = 2.0\n",
    "om_noise_coefficient = 0.5\n",
    "\n",
    "belief_updater = RoombaParticleFilter(spf, v_noise_coefficient, om_noise_coefficient);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a QMDP Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1   ] residual:       11.1 | iteration runtime:   9671.627 ms, (      9.67 s total)\n",
      "[Iteration 2   ] residual:       10.5 | iteration runtime:   9747.175 ms, (      19.4 s total)\n",
      "[Iteration 3   ] residual:         10 | iteration runtime:   9825.156 ms, (      29.2 s total)\n",
      "[Iteration 4   ] residual:       9.52 | iteration runtime:   9691.763 ms, (      38.9 s total)\n",
      "[Iteration 5   ] residual:       9.04 | iteration runtime:   9693.303 ms, (      48.6 s total)\n",
      "[Iteration 6   ] residual:       8.59 | iteration runtime:   9662.353 ms, (      58.3 s total)\n",
      "[Iteration 7   ] residual:       8.16 | iteration runtime:   9760.172 ms, (      68.1 s total)\n",
      "[Iteration 8   ] residual:       7.75 | iteration runtime:   9677.182 ms, (      77.7 s total)\n",
      "[Iteration 9   ] residual:       7.36 | iteration runtime:   9701.439 ms, (      87.4 s total)\n",
      "[Iteration 10  ] residual:       1.14 | iteration runtime:   9816.119 ms, (      97.2 s total)\n",
      "[Iteration 11  ] residual:       1.04 | iteration runtime:   9713.918 ms, (       107 s total)\n",
      "[Iteration 12  ] residual:      0.952 | iteration runtime:   9670.129 ms, (       117 s total)\n",
      "[Iteration 13  ] residual:      0.904 | iteration runtime:   9697.319 ms, (       126 s total)\n",
      "[Iteration 14  ] residual:      0.859 | iteration runtime:   9679.997 ms, (       136 s total)\n",
      "[Iteration 15  ] residual:      0.816 | iteration runtime:   9647.645 ms, (       146 s total)\n",
      "[Iteration 16  ] residual:      0.775 | iteration runtime:   9776.915 ms, (       155 s total)\n",
      "[Iteration 17  ] residual:      0.507 | iteration runtime:   9725.666 ms, (       165 s total)\n",
      "[Iteration 18  ] residual:       0.46 | iteration runtime:   9717.667 ms, (       175 s total)\n",
      "[Iteration 19  ] residual:      0.437 | iteration runtime:   9728.820 ms, (       185 s total)\n",
      "[Iteration 20  ] residual:      0.415 | iteration runtime:   9756.705 ms, (       194 s total)\n",
      "[Iteration 21  ] residual:      0.394 | iteration runtime:   9658.921 ms, (       204 s total)\n",
      "[Iteration 22  ] residual:      0.375 | iteration runtime:   9709.879 ms, (       214 s total)\n",
      "[Iteration 23  ] residual:      0.356 | iteration runtime:   9822.728 ms, (       224 s total)\n",
      "[Iteration 24  ] residual:      0.338 | iteration runtime:  10116.239 ms, (       234 s total)\n",
      "[Iteration 25  ] residual:      0.321 | iteration runtime:   9792.504 ms, (       243 s total)\n",
      "[Iteration 26  ] residual:      0.305 | iteration runtime:  11013.364 ms, (       254 s total)\n",
      "[Iteration 27  ] residual:       0.29 | iteration runtime:   9773.772 ms, (       264 s total)\n",
      "[Iteration 28  ] residual:      0.025 | iteration runtime:   9895.298 ms, (       274 s total)\n",
      "[Iteration 29  ] residual:     0.0238 | iteration runtime:   9770.219 ms, (       284 s total)\n",
      "[Iteration 30  ] residual:     0.0226 | iteration runtime:   9927.907 ms, (       294 s total)\n",
      "[Iteration 31  ] residual:     0.0215 | iteration runtime:   9792.456 ms, (       304 s total)\n",
      "[Iteration 32  ] residual:     0.0204 | iteration runtime:  12358.657 ms, (       316 s total)\n",
      "[Iteration 33  ] residual:     0.0194 | iteration runtime:  10389.899 ms, (       326 s total)\n",
      "[Iteration 34  ] residual:     0.0184 | iteration runtime:   9780.054 ms, (       336 s total)\n",
      "[Iteration 35  ] residual:     0.0175 | iteration runtime:   9783.756 ms, (       346 s total)\n",
      "[Iteration 36  ] residual:     0.0166 | iteration runtime:   9783.245 ms, (       356 s total)\n",
      "[Iteration 37  ] residual:     0.0158 | iteration runtime:   9788.921 ms, (       366 s total)\n",
      "[Iteration 38  ] residual:      0.015 | iteration runtime:  10036.901 ms, (       376 s total)\n",
      "[Iteration 39  ] residual:     0.0142 | iteration runtime:   9842.427 ms, (       385 s total)\n",
      "[Iteration 40  ] residual:     0.0135 | iteration runtime:  10331.109 ms, (       396 s total)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlphaVectorPolicy{RoombaPOMDP{Bumper,Bool},RoombaAct}(RoombaPOMDP{Bumper,Bool}(Bumper(), RoombaMDP{DiscreteRoombaStateSpace,Array{RoombaAct,1}}\n",
       "  v_max: Float64 10.0\n",
       "  om_max: Float64 1.0\n",
       "  dt: Float64 0.5\n",
       "  contact_pen: Float64 -1.0\n",
       "  time_pen: Float64 -0.1\n",
       "  goal_reward: Float64 10.0\n",
       "  stairs_penalty: Float64 -10.0\n",
       "  config: Int64 2\n",
       "  sspace: DiscreteRoombaStateSpace\n",
       "  room: AA228FinalProject.Room\n",
       "  aspace: Array{RoombaAct}((3,))\n",
       "  _amap: Dict{RoombaAct,Int64}\n",
       "), Array{Float64,1}[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], RoombaAct[[3.0, -0.5], [3.0, 0.0], [3.0, 0.5]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vlist = [3.0]\n",
    "omlist = [-0.5,0,0.5]\n",
    "aspace = vec(collect(RoombaAct(v, om) for v in vlist, om in omlist))\n",
    "\n",
    "num_x_pts = 100\n",
    "num_y_pts = 100\n",
    "num_th_pts = 8\n",
    "sspace = DiscreteRoombaStateSpace(num_x_pts,num_y_pts,num_th_pts)\n",
    "\n",
    "mDiscrete = RoombaPOMDP(sensor=sensor, mdp=RoombaMDP(config=config, aspace=aspace, sspace=sspace))\n",
    "\n",
    "# initialize a solver and compute a policy\n",
    "solver = QMDPSolver(max_iterations=40,\n",
    "                    tolerance=1e-3,\n",
    "                    verbose=true) # from QMDP\n",
    "QMDPPolicy = solve(solver, mDiscrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a policy\n",
    "\n",
    "Here we demonstrate how to define a naive policy that attempts navigate the Roomba to the goal. The heuristic policy we define here first spins around for 25 time-steps in order to perform localization, then follows a simple proprtional control law that navigates the robot in the direction of the goal state (note that this policy fails if there is a wall in the way).\n",
    "\n",
    "First we create a struct that subtypes the Policy abstract type, defined in the package ```POMDPPolicies.jl```. Here, we can also define certain parameters, such as a variable tracking the current time-step.\n",
    "\n",
    "Next, we define a function that can take in our policy and the belief state and return the desired action. We do this by defining a new ```POMDPs.action``` function that will work with our policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-20.0, -20.0]"
     ]
    }
   ],
   "source": [
    "# Define the policy to test\n",
    "mutable struct ToEnd <: Policy\n",
    "    ts::Int64 # to track the current time-step.\n",
    "end\n",
    "\n",
    "# extract goal for heuristic controller\n",
    "goal_xy = get_goal_xy(m)\n",
    "print(goal_xy)\n",
    "\n",
    "# define a new function that takes in the policy struct and current belief,\n",
    "# and returns the desired action\n",
    "function POMDPs.action(p::ToEnd, b::ParticleCollection{RoombaState})\n",
    "    p.ts += 1\n",
    "#     if length(particles(b)) == 0\n",
    "#         return action(QMDPPolicy, uniform_belief(mDiscrete))\n",
    "#     end\n",
    "    if AA228FinalProject.wall_contact(m,particles(b)[1])\n",
    "        return RoombaAct(3.0,-pi)\n",
    "    end\n",
    "\n",
    "    a = action(QMDPPolicy, b)\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation and rendering\n",
    "\n",
    "Here, we will demonstrate how to seed the environment, run a simulation, and render the simulation. To render the simulation, we use the ```Gtk``` package. \n",
    "\n",
    "The simulation is carried out using the ```stepthrough``` function defined in the package ```POMDPSimulators.jl```. During a simulation, a window will open that renders the scene. It may be hidden behind other windows on your desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first seed the environment\n",
    "Random.seed!(5)\n",
    "\n",
    "# reset the policy\n",
    "p = ToEnd(0) # here, the argument sets the time-steps elapsed to 0\n",
    "# for (t, step) in enumerate(stepthrough(m, p, belief_updater, max_steps=100))\n",
    "#     print(\"hi\")\n",
    "# end\n",
    "# run the simulation\n",
    "c = @GtkCanvas()\n",
    "win = GtkWindow(c, \"Roomba Environment\", 600, 600)\n",
    "for (t, step) in enumerate(stepthrough(m, p, belief_updater, max_steps=100))\n",
    "    @guarded draw(c) do widget\n",
    "        \n",
    "        # the following lines render the room, the particles, and the roomba\n",
    "        ctx = getgc(c)\n",
    "        set_source_rgb(ctx,1,1,1)\n",
    "        paint(ctx)\n",
    "        render(ctx, m, step)\n",
    "        \n",
    "        # render some information that can help with debugging\n",
    "        # here, we render the time-step, the state, and the observation\n",
    "        move_to(ctx,300,400)\n",
    "        show_text(ctx, @sprintf(\"t=%d, state=%s, o=%.3f\",t,string(step.s),step.o))\n",
    "    end\n",
    "    show(c)\n",
    "    sleep(0.1) # to slow down the simulation\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "Here, we demonstate a simple evaluation of the policy's performance for a few random seeds. This is meant to serve only as an example, and we encourage you to develop your own evaluation metrics.\n",
    "\n",
    "We intialize the robot using five different random seeds, and simulate its performance for 100 time-steps. We then sum the rewards experienced during its interaction with the environment and track this total reward for the five trials.\n",
    "Finally, we report the mean and standard error for the total reward. The standard error is the standard deviation of a sample set divided by the square root of the number of samples, and represents the uncertainty in the estimate of the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "reached goal\n",
      "4\n",
      "reached goal\n",
      "5\n",
      "reached goal\n",
      "6\n",
      "reached goal\n",
      "7\n",
      "8\n",
      "reached goal\n",
      "9\n",
      "reached goal\n",
      "10\n",
      "reached goal\n",
      "11\n",
      "reached goal\n",
      "12\n",
      "13\n",
      "reached goal\n",
      "14\n",
      "reached goal\n",
      "15\n",
      "reached goal\n",
      "16\n",
      "reached goal\n",
      "17\n"
     ]
    },
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 0-element Array{RoombaState,1} at index [1]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 0-element Array{RoombaState,1} at index [1]",
      "",
      "Stacktrace:",
      " [1] getindex at .\\array.jl:731 [inlined]",
      " [2] action(::ToEnd, ::ParticleCollection{RoombaState}) at .\\In[37]:17",
      " [3] action_info at C:\\Users\\zacfa\\.julia\\packages\\POMDPModelTools\\eHEjm\\src\\info.jl:30 [inlined]",
      " [4] iterate(::POMDPSimulators.POMDPSimIterator{(:s, :a, :r, :sp, :t, :i, :ai, :b, :o, :bp, :ui),RoombaPOMDP{Bumper,Bool},ToEnd,RoombaParticleFilter,MersenneTwister,ParticleCollection{RoombaState},RoombaState}, ::Tuple{Int64,RoombaState,ParticleCollection{RoombaState}}) at C:\\Users\\zacfa\\.julia\\packages\\POMDPSimulators\\xyfJM\\src\\stepthrough.jl:102",
      " [5] top-level scope at .\\In[44]:21"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "\n",
    "total_rewards = []\n",
    "num_success = 0\n",
    "num_seeds = 10\n",
    "\n",
    "for exp = 1:num_seeds\n",
    "    println(string(exp))\n",
    "    \n",
    "    Random.seed!(exp)\n",
    "    #srand(exp)\n",
    "    \n",
    "    p = ToEnd(0)\n",
    "    traj_rewards = 0\n",
    "    for step in stepthrough(m,p,belief_updater, max_steps=100)\n",
    "        traj_rewards += step.r\n",
    "        if step.r > 5\n",
    "            println(\"reached goal\")\n",
    "            num_success += 1\n",
    "            push!(total_rewards, traj_rewards)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "#     traj_rewards = sum([step.r for step in stepthrough(m,p,belief_updater, max_steps=300)])\n",
    "    \n",
    "#     push!(total_rewards, traj_rewards)\n",
    "end\n",
    "\n",
    "success_rate = (num_success*1.0)/num_seeds\n",
    "mtr = mean(total_rewards)\n",
    "score = success_rate*success_rate*mtr\n",
    "@printf(\"Percent that reached goal: %.3f%%\", success_rate*100)\n",
    "println()\n",
    "@printf(\"Mean Total Reward: %.3f\", mtr)\n",
    "println()\n",
    "x = \n",
    "@printf(\"Score: %.3f\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
